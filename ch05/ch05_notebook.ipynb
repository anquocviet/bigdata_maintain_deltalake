{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17156fd5-da02-43de-96e9-eedd144d4b58",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Chapter 5: Maintaining your Delta Lake\n",
    "> The following exercises use the New York Times [Covid-19 NYT Dataset](https://github.com/delta-io/delta-docs/tree/main/static/quickstart_docker/rs/data/COVID-19_NYT).\n",
    "\n",
    "The dataset can be found in the `delta_quickstart` docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ea01058-0674-4231-9e2d-bbcf10f81bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import col, desc, to_date\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "056cfece-8b8d-45f5-9abc-4d0ae3fa43ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS default.covid_nyt (\n",
    "  date DATE\n",
    ") USING DELTA\n",
    "TBLPROPERTIES('delta.logRetentionDuration'='interval 7 days');\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03c59705-37a5-4acb-9e85-7bfcc422cd1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  default|covid_nyt|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8691f12a-e961-4f6a-bed3-b404ab4ffad5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# will be empty on the first run. this is expected\n",
    "len(spark.table(\"default.covid_nyt\").inputFiles())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7c4e08-d4b4-4471-bbc8-07aba57dd1e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# uncomment if you'd like to begin again\n",
    "#spark.sql(\"drop table default.covid_nyt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d37d79-4184-4d8f-b1a9-25fe08463ab6",
   "metadata": {},
   "source": [
    "## Start Populating the Table\n",
    "> The next three commands are used to show Schema Evolution and Validation with Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8e2088c-f84f-4973-ab71-9051a710d31f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table default.covid_nyt already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Populate the Table reading the Parquet covid_nyc Data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# note: this will fail on the first run, and that is okay\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m (\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/opt/spark/work-dir/rs/data/COVID-19_NYT/*.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_date\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myyyy-MM-dd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault.covid_nyt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:1041\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1041\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table default.covid_nyt already exists"
     ]
    }
   ],
   "source": [
    "# Populate the Table reading the Parquet covid_nyc Data\n",
    "# note: this will fail on the first run, and that is okay\n",
    "(spark.read\n",
    "      .format(\"parquet\")\n",
    "      .load(\"/opt/spark/work-dir/rs/data/COVID-19_NYT/*.parquet\")\n",
    "      .withColumn(\"date\", to_date(\"date\", \"yyyy-MM-dd\"))\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .saveAsTable(\"default.covid_nyt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75ae71d8-3659-437e-afca-b4cfc265e822",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "A schema mismatch detected when writing to the Delta table (Table ID: 6ef3797a-1647-4ef0-9f22-8547d2dd0dc7).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- date: date (nullable = true)\n\n\nData schema:\nroot\n-- date: date (nullable = true)\n-- county: string (nullable = true)\n-- state: string (nullable = true)\n-- fips: integer (nullable = true)\n-- cases: integer (nullable = true)\n-- deaths: integer (nullable = true)\n\n         ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# one step closer, there is still something missing...\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# and yes, this operation still fails... if only...\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m (\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/opt/spark/work-dir/rs/data/COVID-19_NYT/*.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_date\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myyyy-MM-dd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault.covid_nyt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:1041\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1041\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: A schema mismatch detected when writing to the Delta table (Table ID: 6ef3797a-1647-4ef0-9f22-8547d2dd0dc7).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- date: date (nullable = true)\n\n\nData schema:\nroot\n-- date: date (nullable = true)\n-- county: string (nullable = true)\n-- state: string (nullable = true)\n-- fips: integer (nullable = true)\n-- cases: integer (nullable = true)\n-- deaths: integer (nullable = true)\n\n         "
     ]
    }
   ],
   "source": [
    "# one step closer, there is still something missing...\n",
    "# and yes, this operation still fails... if only...\n",
    "(spark.read\n",
    "      .format(\"parquet\")\n",
    "      .load(\"/opt/spark/work-dir/rs/data/COVID-19_NYT/*.parquet\")\n",
    "      .withColumn(\"date\", to_date(\"date\", \"yyyy-MM-dd\"))\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"append\")\n",
    "      .saveAsTable(\"default.covid_nyt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8061db2c-fc2e-4674-8e48-e62e06902d13",
   "metadata": {},
   "source": [
    "## Schema Evolution: Handle Automatically\n",
    "If you trust the upstream data source (provider) then you can add the `option(\"mergeSchema\", \"true\")`. Otherwise, it is better to specifically select a subset of the columns you expected to see. In this example use case, the only known column is `date`, so it is fairly safe to power ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "196c1e23-48be-45e1-b627-9894991e53e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/21 05:47:44 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evolve the Schema. (Showcases how to auto-merge changes to the schema)\n",
    "# note: if you can trust the upstream, then this option is perfectly fine\n",
    "# however, if you don't trust the upstream, then it is good to opt-in to the \n",
    "# changing columns.\n",
    "\n",
    "(spark.read\n",
    "      .format(\"parquet\")\n",
    "      .load(\"/opt/spark/work-dir/rs/data/COVID-19_NYT/*.parquet\")\n",
    "      .withColumn(\"date\", to_date(\"date\", \"yyyy-MM-dd\"))\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"append\")\n",
    "      .option(\"mergeSchema\", \"true\") # tu dong cap nhat schema\n",
    "      .saveAsTable(\"default.covid_nyt\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7da844a5-7999-43d2-ad6e-76a0e2b5396f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1111930"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.table(\"default.covid_nyt\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4931837-6282-4ac3-96f9-6febd15b96c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Alternatives to Auto Schema Evolution - Tự động nâng cấp schema\n",
    "In the previous case, we used `.option(\"mergeSchema\", \"true\")` to modify the behavior of the Delta Lake writer. While this option simplifies how we evolve our Delta Lake table schemas, it comes at the price of not being fully aware of the changes to our table schema. In the case where there are unknown columns being introduced from an upstream source, you'll want to know which columns are intended to bring forward, and which columns can be safely ignored.\n",
    "\n",
    "## Intentionally Adding Columns with Alter Table - Cố tình thêm cột cho bảng đã thay đổi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "723603f6-48bd-424b-96b5-750828a0f3b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot add column, because county already exists in root\n |-- date: date (nullable = true)\n |-- county: string (nullable = true)\n |-- state: string (nullable = true)\n |-- fips: integer (nullable = true)\n |-- cases: integer (nullable = true)\n |-- deaths: integer (nullable = true)\n; line 2 pos 0;\nAddColumns [QualifiedColType(None,county,StringType,true,None,None), QualifiedColType(None,state,StringType,true,None,None), QualifiedColType(None,fips,IntegerType,true,None,None), QualifiedColType(None,cases,IntegerType,true,None,None), QualifiedColType(None,deaths,IntegerType,true,None,None)]\n+- ResolvedTable org.apache.spark.sql.delta.catalog.DeltaCatalog@5ff90a57, default.covid_nyt, DeltaTableV2(org.apache.spark.sql.SparkSession@3deea508,file:/opt/spark/work-dir/ch6/spark-warehouse/covid_nyt,Some(CatalogTable(\nDatabase: default\nTable: covid_nyt\nOwner: NBuser\nCreated Time: Thu Nov 21 05:47:05 UTC 2024\nLast Access: UNKNOWN\nCreated By: Spark 3.3.2\nType: MANAGED\nProvider: delta\nLocation: file:/opt/spark/work-dir/ch6/spark-warehouse/covid_nyt\nSerde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\nInputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\nOutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\nPartition Provider: Catalog)),Some(default.covid_nyt),None,Map(),org.apache.spark.sql.util.CaseInsensitiveStringMap@1f), [date#1615, county#1616, state#1617, fips#1618, cases#1619, deaths#1620]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# manually set the columns. This is an example of intentional opt-in to the new columns outside of '.option(\"mergeSchema\", \"true\")`. \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Note: this can be run once, afterwards the ADD columns will fail since they already exist\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43mALTER TABLE default.covid_nyt \u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43mADD columns (\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43m  county STRING,\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43m  state STRING,\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43m  fips INT,\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;43m  cases INT,\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;43m  deaths INT\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;43m);\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# notice how we are only using `.mode(\"append\")` and explicitly add `.option(\"mergeSchema\", \"false\")`. \u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# this is how we stop unwanted columns from being freely added to our Delta Lake tables. It comes at the cost of raising exceptions and failing the job.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# a failed job might seem like a bad option, but it is the cheaper option since you are intentionally blocking unknown data from flowing into your tables. \u001b[39;00m\n\u001b[1;32m     16\u001b[0m (spark\u001b[38;5;241m.\u001b[39mread\n\u001b[1;32m     17\u001b[0m       \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m       \u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/opt/spark/work-dir/rs/data/COVID-19_NYT/*.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m       \u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault.covid_nyt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1034\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     sqlQuery \u001b[38;5;241m=\u001b[39m formatter\u001b[38;5;241m.\u001b[39mformat(sqlQuery, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot add column, because county already exists in root\n |-- date: date (nullable = true)\n |-- county: string (nullable = true)\n |-- state: string (nullable = true)\n |-- fips: integer (nullable = true)\n |-- cases: integer (nullable = true)\n |-- deaths: integer (nullable = true)\n; line 2 pos 0;\nAddColumns [QualifiedColType(None,county,StringType,true,None,None), QualifiedColType(None,state,StringType,true,None,None), QualifiedColType(None,fips,IntegerType,true,None,None), QualifiedColType(None,cases,IntegerType,true,None,None), QualifiedColType(None,deaths,IntegerType,true,None,None)]\n+- ResolvedTable org.apache.spark.sql.delta.catalog.DeltaCatalog@5ff90a57, default.covid_nyt, DeltaTableV2(org.apache.spark.sql.SparkSession@3deea508,file:/opt/spark/work-dir/ch6/spark-warehouse/covid_nyt,Some(CatalogTable(\nDatabase: default\nTable: covid_nyt\nOwner: NBuser\nCreated Time: Thu Nov 21 05:47:05 UTC 2024\nLast Access: UNKNOWN\nCreated By: Spark 3.3.2\nType: MANAGED\nProvider: delta\nLocation: file:/opt/spark/work-dir/ch6/spark-warehouse/covid_nyt\nSerde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\nInputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\nOutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\nPartition Provider: Catalog)),Some(default.covid_nyt),None,Map(),org.apache.spark.sql.util.CaseInsensitiveStringMap@1f), [date#1615, county#1616, state#1617, fips#1618, cases#1619, deaths#1620]\n"
     ]
    }
   ],
   "source": [
    "# manually set the columns. This is an example of intentional opt-in to the new columns outside of '.option(\"mergeSchema\", \"true\")`. \n",
    "# Note: this can be run once, afterwards the ADD columns will fail since they already exist\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE default.covid_nyt \n",
    "ADD columns (\n",
    "  county STRING,\n",
    "  state STRING,\n",
    "  fips INT,\n",
    "  cases INT,\n",
    "  deaths INT\n",
    ");\n",
    "\"\"\")\n",
    "# notice how we are only using `.mode(\"append\")` and explicitly add `.option(\"mergeSchema\", \"false\")`. \n",
    "# this is how we stop unwanted columns from being freely added to our Delta Lake tables. It comes at the cost of raising exceptions and failing the job.\n",
    "# a failed job might seem like a bad option, but it is the cheaper option since you are intentionally blocking unknown data from flowing into your tables. \n",
    "(spark.read\n",
    "      .format(\"parquet\")\n",
    "      .load(\"/opt/spark/work-dir/rs/data/COVID-19_NYT/*.parquet\")\n",
    "      .withColumn(\"date\", to_date(\"date\", \"yyyy-MM-dd\"))\n",
    "      .write\n",
    "      .format(\"delta\")\n",
    "      .option(\"mergeSchema\", \"false\")\n",
    "      .mode(\"append\")\n",
    "      .saveAsTable(\"default.covid_nyt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87216c69-b742-408f-a6fb-ccac96246b0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                                     |comment|\n",
      "+----------------------------+----------------------------------------------------------------------------------------------+-------+\n",
      "|date                        |date                                                                                          |       |\n",
      "|county                      |string                                                                                        |       |\n",
      "|state                       |string                                                                                        |       |\n",
      "|fips                        |int                                                                                           |       |\n",
      "|cases                       |int                                                                                           |       |\n",
      "|deaths                      |int                                                                                           |       |\n",
      "|                            |                                                                                              |       |\n",
      "|# Partitioning              |                                                                                              |       |\n",
      "|Not partitioned             |                                                                                              |       |\n",
      "|                            |                                                                                              |       |\n",
      "|# Detailed Table Information|                                                                                              |       |\n",
      "|Name                        |default.covid_nyt                                                                             |       |\n",
      "|Location                    |file:/opt/spark/work-dir/ch6/spark-warehouse/covid_nyt                                        |       |\n",
      "|Provider                    |delta                                                                                         |       |\n",
      "|Owner                       |NBuser                                                                                        |       |\n",
      "|Table Properties            |[delta.logRetentionDuration=interval 7 days,delta.minReaderVersion=1,delta.minWriterVersion=2]|       |\n",
      "+----------------------------+----------------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe extended default.covid_nyt\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63bf64e2-e0ea-4aeb-bdf5-352891a2cb1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------------+-----+-----+------+\n",
      "|      date| county|        state| fips|cases|deaths|\n",
      "+----------+-------+-------------+-----+-----+------+\n",
      "|2020-10-10| Tucker|West Virginia|54093|   46|     0|\n",
      "|2020-10-10|  Tyler|West Virginia|54095|   20|     0|\n",
      "|2020-10-10| Upshur|West Virginia|54097|  175|     0|\n",
      "|2020-10-10|  Wayne|West Virginia|54099|  437|    11|\n",
      "|2020-10-10|Webster|West Virginia|54101|    9|     0|\n",
      "|2020-10-10| Wetzel|West Virginia|54103|   68|     0|\n",
      "|2020-10-10|   Wirt|West Virginia|54105|   20|     0|\n",
      "|2020-10-10|   Wood|West Virginia|54107|  408|     6|\n",
      "|2020-10-10|Wyoming|West Virginia|54109|  136|     5|\n",
      "|2020-10-10|  Adams|    Wisconsin|55001|  316|     4|\n",
      "+----------+-------+-------------+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from default.covid_nyt limit 10\").show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d76385-bf01-4079-93cc-6690d06a3a09",
   "metadata": {},
   "source": [
    "# Adding and Modifying Table Properties - Thêm và sửa lại các thuộc tính của bảng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff1a15b5-6b13-4e75-8faf-aa3b93c36b03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "  ALTER TABLE default.covid_nyt \n",
    "  SET TBLPROPERTIES (\n",
    "    'catalog.team_name'='dldg_authors',\n",
    "    'catalog.engineering.comms.slack'='https://delta-users.slack.com/archives/CG9LR6LN4',\n",
    "    'catalog.engineering.comms.email'='dldg_authors@gmail.com',\n",
    "    'catalog.table.classification'='all-access'\n",
    "  )\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35d24afa-7fa7-4ee0-bdbd-94ff01f71e35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------------+\n",
      "|version|           timestamp|        operation|\n",
      "+-------+--------------------+-----------------+\n",
      "|      3|2024-11-21 09:05:...|SET TBLPROPERTIES|\n",
      "|      2|2024-11-21 06:14:...|SET TBLPROPERTIES|\n",
      "|      1|2024-11-21 05:47:...|            WRITE|\n",
      "|      0|2024-11-21 05:46:...|     CREATE TABLE|\n",
      "+-------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view the table history\n",
    "from delta.tables import DeltaTable\n",
    "dt = DeltaTable.forName(spark, 'default.covid_nyt')\n",
    "dt.history(10).select(\"version\", \"timestamp\", \"operation\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "491e0cda-e0ac-4920-8d37-1faaacbf8dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|properties                                                                                                                                                                                                                                                                    |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{catalog.table.classification -> all-access, catalog.engineering.comms.email -> dldg_authors@gmail.com, delta.logRetentionDuration -> interval 7 days, catalog.team_name -> dldg_authors, catalog.engineering.comms.slack -> https://delta-users.slack.com/archives/CG9LR6LN4}|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use DeltaTable to view\n",
    "dt.detail().select(\"properties\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "961a5c66-5183-43f7-8b79-3dfddae8d486",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------------------------------------------------+\n",
      "|key                            |value                                           |\n",
      "+-------------------------------+------------------------------------------------+\n",
      "|catalog.engineering.comms.email|dldg_authors@gmail.com                          |\n",
      "|catalog.engineering.comms.slack|https://delta-users.slack.com/archives/CG9LR6LN4|\n",
      "|catalog.table.classification   |all-access                                      |\n",
      "|catalog.team_name              |dldg_authors                                    |\n",
      "|delta.logRetentionDuration     |interval 7 days                                 |\n",
      "|delta.minReaderVersion         |1                                               |\n",
      "|delta.minWriterVersion         |2                                               |\n",
      "+-------------------------------+------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view the table properties\n",
    "spark.sql(\"show tblproperties default.covid_nyt\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266b9a32-3cb9-425f-a925-70aa6faafd51",
   "metadata": {},
   "source": [
    "## Removing Table Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ca0dfad-60a5-4dfd-a20d-b4a10888cf8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are setting a property: delta.loRgetentionDuratio that is not recognized by this version of Delta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add incorrect table property\n",
    "# which is blocked by default\n",
    "spark.conf.set(\"spark.databricks.delta.allowArbitraryProperties.enabled\",\"true\")\n",
    "# now we can make a mistake\n",
    "spark.sql(\"\"\"\n",
    "  ALTER TABLE default.covid_nyt \n",
    "  SET TBLPROPERTIES (\n",
    "    'delta.loRgetentionDuratio'='interval 7 days'\n",
    "  )\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b2a27b6-6b8d-475d-a357-8dfd1d2ae46a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# luckily, we can remove the unwanted table property using UNSET\n",
    "spark.sql(\"\"\"\n",
    "  ALTER TABLE default.covid_nyt \n",
    "  UNSET TBLPROPERTIES ('delta.loRgetentionDuratio')\n",
    "\"\"\")\n",
    "# now that we are done, let's just add back the safe guard again\n",
    "spark.conf.set(\"spark.databricks.delta.allowArbitraryProperties.enabled\",\"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283f5c63-f40a-490a-a22d-740ee85b10c8",
   "metadata": {},
   "source": [
    "## Delta Table Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f56ac8e-77df-4d26-95a3-131b744228f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/21 09:33:14 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/11/21 09:33:14 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/11/21 09:33:17 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "24/11/21 09:33:17 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.228.2\n",
      "24/11/21 09:33:19 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<delta.tables.DeltaTable at 0x7f3076bcd6d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating the Small File Problem\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "(DeltaTable.createIfNotExists(spark)\n",
    "    .tableName(\"default.nonoptimal_covid_nyt\")\n",
    "    .property(\"description\", \"table to be optimized\")\n",
    "    .property(\"catalog.team_name\", \"dldg_authors\")\n",
    "    .property(\"catalog.engineering.comms.slack\",\n",
    "\t\"https://delta-users.slack.com/archives/CG9LR6LN4\")\n",
    "    .property(\"catalog.engineering.comms.email\",\"dldg_authors@gmail.com\")\n",
    "    .property(\"catalog.table.classification\",\"all-access\")\n",
    "    .addColumn(\"date\", \"DATE\")\n",
    "    .addColumn(\"county\", \"STRING\")\n",
    "    .addColumn(\"state\", \"STRING\")\n",
    "    .addColumn(\"fips\", \"INT\")\n",
    "    .addColumn(\"cases\", \"INT\")\n",
    "    .addColumn(\"deaths\", \"INT\")\n",
    "    .execute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41f8ee4a-4628-411d-8379-6707d67605dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spark.sql(\"drop table default.nonoptimal_covid_nyt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6155308-2f74-4097-868f-fd3c026c6895",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/21 09:33:41 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                      (0 + 8) / 9000]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/21 09:33:53 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/21 09:36:08 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "24/11/21 09:36:08 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/11/21 09:36:08 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/11/21 09:36:08 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    }
   ],
   "source": [
    "# you can remove `repartition(9000)` and add write...option('maxRecordsPerFile`, 10000)\n",
    "# to generate more files using the DataFrameWriter\n",
    "(spark\n",
    "   .table(\"default.covid_nyt\")\n",
    "   .repartition(9000)\n",
    "   .write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"overwrite\")\n",
    "   # .option(\"maxRecordsPerFile\", 1000)\n",
    "   .saveAsTable(\"default.nonoptimal_covid_nyt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585f9428-c1b9-4c9b-a359-f4433d09b58c",
   "metadata": {},
   "source": [
    "## Using Optimize to Fix the Small Files Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f742e24-531c-4a22-aefa-0647493a2315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set the maxFileSize to a bin-size for optimize\n",
    "spark.conf.set(\"spark.databricks.delta.optimize.maxFileSize\", 1024*1024*1024)\n",
    "(\n",
    "    DeltaTable.forName(spark, \"default.nonoptimal_covid_nyt\")\n",
    "    .optimize()\n",
    "    .executeCompaction()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9b9463-d642-45fc-ae83-ca4c17ad00ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Viewing the results of Optimize\n",
    "from pyspark.sql.functions import col\n",
    "(\n",
    "    DeltaTable.forName(spark, \"default.nonoptimal_covid_nyt\")\n",
    "    .history(10)\n",
    "    .where(col(\"operation\") == \"OPTIMIZE\")\n",
    "    .select(\"version\", \"timestamp\", \"operation\", \"operationMetrics.numRemovedFiles\", \"operationMetrics.numAddedFiles\")\n",
    "    .show(truncate=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79bd28d-4b45-424d-a3a9-afeff7186c6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rewind and try again\n",
    "# note: the table version of the OPTIMIZE operation needs to be referenced to take the prior version\n",
    "#(DeltaTable.forName(spark, \"default.nonoptimal_covid_nyt\").restoreToVersion(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ff55f5-8d55-42b4-b9e2-289b7f3fdf54",
   "metadata": {},
   "source": [
    "## Partitioning, Repartitioning, and Default Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39850430-2c93-4812-8742-be1cb33ab51a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.types import DateType\n",
    "(DeltaTable.createIfNotExists(spark)\n",
    "    .tableName(\"default.covid_nyt_by_date\")\n",
    "    .property(\"description\", \"table with default partitions\")\n",
    "    .property(\"catalog.team_name\", \"dldg_authors\")\n",
    "    .property(\"catalog.engineering.comms.slack\",\n",
    "\t\"https://delta-users.slack.com/archives/CG9LR6LN4\")\n",
    "    .property(\"catalog.engineering.comms.email\",\"dldg_authors@gmail.com\")\n",
    "    .property(\"catalog.table.classification\",\"all-access\")\n",
    "    .addColumn(\"date\", DateType(), nullable=False)\n",
    "    .addColumn(\"county\", \"STRING\")\n",
    "    .addColumn(\"state\", \"STRING\")\n",
    "    .addColumn(\"fips\", \"INT\")\n",
    "    .addColumn(\"cases\", \"INT\")\n",
    "    .addColumn(\"deaths\", \"INT\")\n",
    "    .partitionedBy(\"date\")\n",
    "    .execute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862a2d0e-dd94-402d-8e43-df2a696c47b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"drop table default.covid_nyt_by_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38da1e9a-f9c5-4fda-8926-f9f671a3d6b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use our non-partitioned source table to populate our partitioned table (automatically)\n",
    "(\n",
    "    spark\n",
    "    .table(\"default.covid_nyt\")\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .option(\"mergeSchema\", \"false\")\n",
    "    .saveAsTable(\"default.covid_nyt_by_date\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f10adcd-27a2-418c-bf63-0b2a49956710",
   "metadata": {},
   "source": [
    "## Viewing the Partition Metadata of our Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9df68c-a8a0-4036-bb6a-96874a8dea72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"describe extended default.covid_nyt_by_date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a49de1f-eddf-4812-9b61-56f76c8973c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# view the table metadata as a json blob\n",
    "\n",
    "DeltaTable.forName(spark, \"default.covid_nyt_by_date\").detail().toJSON().collect()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc472291-a327-4f26-8f47-997ef20b5bbb",
   "metadata": {},
   "source": [
    "# Create Bronze and Silver Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f689b-6bd0-487f-806e-a19cdfe1dae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"show databases;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8952dd29-ced9-4c50-a825-f6460b560a7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We need to first create two databases (schemas) in our Hive metastore, or Unity Catalog.\n",
    "# If using Unity Catalog, you can prefix <catalog>.<schema>.<table>\n",
    "# With Hive, you can only use <schema>.<table>\n",
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronze\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e967846a-3ecd-4994-baa7-b578920e05be",
   "metadata": {},
   "source": [
    "## COPY (CLONE) Tables between Databases (Schemas)\n",
    "> We will be copying `default.covid_nyt_by_date` using DEEP CLONE into `bronze.covid_nyt_by_date` and `silver.covid_nyt_by_date`\n",
    "> This functionality is available in the Databricks runtime as [CLONE](https://docs.databricks.com/delta/clone.html). [Shallow Cloning](https://docs.delta.io/latest/delta-utility.html#shallow-clone-a-delta-table) is available at the time of writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d27fa4-c603-4e55-8665-c2553a60923b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# slim version of https://github.com/MrPowers/mack/blob/main/mack/__init__.py#L288\n",
    "def copy_table(delta_table: DeltaTable, target_table: str):\n",
    "    details = (\n",
    "        delta_table\n",
    "        .detail()\n",
    "        .select(\"partitionColumns\", \"properties\")\n",
    "        .collect()[0]\n",
    "    )\n",
    "    (\n",
    "        table_to_copy.toDF().write.format(\"delta\")\n",
    "        .partitionBy(details[\"partitionColumns\"])\n",
    "        .options(**details[\"properties\"])\n",
    "        .saveAsTable(target_table)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df43d80a-9a3c-4022-a98a-5d3be6a2f0f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copy the default table and write into both bronze and silver\n",
    "table_to_copy = DeltaTable.forName(spark, \"default.covid_nyt_by_date\")\n",
    "bronze_table = \"bronze.covid_nyt_by_date\"\n",
    "silver_table = \"silver.covid_nyt_by_date\"\n",
    "\n",
    "copy_table(table_to_copy, bronze_table)\n",
    "copy_table(table_to_copy, silver_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72410b39-a731-44ee-911d-1c35b935aa03",
   "metadata": {},
   "source": [
    "## Using Shallow Clone to Create a Metadata-Only Copy of a Table\n",
    "Reference Link: https://docs.delta.io/latest/delta-utility.html#shallow-clone-a-delta-table\n",
    "\n",
    "The next example is extra content outside of the book materials for chapter 6. We'll discover how to shallow clone a table using both the path on disk, as well as from table to table references (for managed tables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee57ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use Shallow Clone to Create a Metadata Only Copy of the Table using the table location on disk\n",
    "src_location = DeltaTable.forName(spark, \"default.covid_nyt_by_date\").detail().first()[\"location\"]\n",
    "dest_location = DeltaTable.forName(spark, \"silver.covid_nyt_by_date\").detail().first()[\"location\"]\n",
    "#print(f\"source_table:{src_location}\\ndestination_table_location:{dest_location}\")\n",
    "\n",
    "src_location_fmt = str(src_location).replace(\"file:\", \"\")\n",
    "# steal the silver.db location from the copy table, and just add _clone to the tablename\n",
    "dest_location_clone = str(dest_location).replace(\"file:\",\"\")+'_clone'\n",
    "#print(f\"src:{src_location_fmt}, dest:{dest_location_clone}\")\n",
    "\n",
    "\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS delta.`{dest_location_clone}` SHALLOW CLONE delta.`{dest_location_clone}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cfefa4-e140-4ecd-86c2-5a2eeaef3678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(\"silver\")\n",
    "spark.catalog.listTables()\n",
    "spark.sql(\"show tables\").show()\n",
    "\n",
    "# On the first pass, without writing to the managed table location, you won't be able to see the new cloned table in the table\n",
    "# list. This is one way to work with cloned data where you are not \"broadcasting\" the table into the managed table space. When you are ready\n",
    "# you can always create a managed table using the location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bf435c-6cf8-4552-8b80-716a2866fa43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# It is worth noting that you can CREATE a managed table over an existing non-managed table.\n",
    "# Observe the WARNING when running the next statement.\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS silver.covid_nyt_by_date_clone SHALLOW CLONE default.covid_nyt_by_date\")\n",
    "\n",
    "# if you try to replace a CLONED table, you will get an exception \n",
    "# (DeltaIllegalStateException): The clone destination table is non-empty: Please TRUNCATE or DELETE before running CLONE...\n",
    "# this is to protect the integrity of the clone, the expectation for a SHALLOW CLONE is that it provides metadata only changes\n",
    "# as the source table is still the reference for the data.\n",
    "\n",
    "# To see the behavior in action, try\n",
    "# spark.sql(\"CREATE OR REPLACE TABLE silver.covid_nyt_by_date_clone SHALLOW CLONE default.covid_nyt_by_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1339c2bf-e596-4e61-9f41-ae30a0a64b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after replacing the table clone, you'll see the table in the local table list\n",
    "spark.catalog.setCurrentDatabase(\"silver\")\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b67a5-fc59-4658-82d4-b99f0a278471",
   "metadata": {},
   "source": [
    "## Removing Partitions using Conditional Delete at the Partition Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c1f788-5c98-486a-a0df-279f07b122c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Remove a partition from the silver table so we can repair the table with our bronze table\n",
    "silver_dt = DeltaTable.forName(spark, \"silver.covid_nyt_by_date\")\n",
    "silver_dt.delete(col(\"date\") == \"2021-02-17\")\n",
    "\n",
    "# Note: (if you delete, and then immediately vacuum, you will not be able to restore your table)\n",
    "# vacuum to remove the physical data from the table\n",
    "#spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\")\n",
    "#silver_dt.vacuum(retentionHours=0)\n",
    "#spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aeb1de-c78d-468c-b799-2ca813bda31d",
   "metadata": {},
   "source": [
    "## Using ReplaceWhere to do Conditional Repairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4aa8993-c330-48c1-a75b-5d43ce3f2f82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/16 21:10:20 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/06/16 21:10:20 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "23/06/16 21:10:20 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/06/16 21:10:20 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    }
   ],
   "source": [
    "recovery_table = spark.table(\"bronze.covid_nyt_by_date\")\n",
    "partition_col = \"date\"\n",
    "table_to_fix = \"silver.covid_nyt_by_date\"\n",
    "\n",
    "(recovery_table.where(col(\"date\") == \"2021-02-17\").write.format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"replaceWhere\", f\"{partition_col} == '2021-02-17'\")\n",
    " .saveAsTable(\"silver.covid_nyt_by_date\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0adf03d-6c18-4941-804a-8be6dbdc51bb",
   "metadata": {},
   "source": [
    "## Restoring Tables to a Prior Version\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd5cdc9-7a1d-4496-b332-acc42d46fff1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dt = DeltaTable.forName(spark, \"silver.covid_nyt_by_date\")\n",
    "dt.history(10).select(\"version\", \"timestamp\", \"operation\").show()\n",
    "dt.restoreToVersion(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40af3f71-a249-4fea-885c-6bc4c896dd30",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cleaning up our Delta Tables using Vacuum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc20a7e6-ce3c-4074-b6aa-5f7b2372bfda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\")\n",
    "DeltaTable.forName(spark, \"default.nonoptimal_covid_nyt\").vacuum(retentionHours=0)\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65b8f1b-6b46-49f2-9678-dd3bf56c0e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select distinct(date) as date from default.covid_nyt_by_date order by date desc\").show(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544c6d12-78ce-4da3-8bf3-057bdae41275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select count(distinct(date)) from default.covid_nyt_by_date\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
